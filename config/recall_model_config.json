{
    "model_path": {
        "value": "microsoft/deberta-v3-xsmall",
        "description": "Path to the model to use for training."
    },
    "use_lora": {
        "value": false,
        "description": "Whether to use LoRA for training. Default is false."
    },
    "lora_r": {
        "value": 8,
        "description": "The rank of the LoRA matrices. Default is 8."
    },
    "lora_alpha": {
        "value": 32,
        "description": "The scaling factor for the LoRA matrices. Default is 32."
    },
    "lora_dropout": {
        "value": 0.05,
        "description": "The dropout rate for the LoRA matrices. Default is 0.05."
    },
    "learning_rate": {
        "value": 1e-4,
        "description": "The learning rate for the model. Default is 1e-4."
    },
    "gradient_checkpointing": {
        "value": true,
        "description": "Whether to use gradient checkpointing. Default is true."
    },
    "hidden_dim": {
        "value": 512,
        "description": "The hidden dimension of the model. Default is 512."
    },
    "num_latents": {
        "value": 512,
        "description": "The number of latents to use for the model. Default is 512."
    },
    "num_heads": {
        "value": 8,
        "description": "The number of attention heads to use for the model. Default is 8."
    },
    "mlp_ratio": {
        "value": 4,
        "description": "The ratio of the MLP dimension to the hidden dimension. Default is 4."
    },
    "output_dim": {
        "value": 1024,
        "description": "The output dimension of the model. Default is 1024."
    },
    "sentence_pooling_method": {
        "value": "cls",
        "description": "The method to use for sentence pooling. Default is 'cls'. Can be 'mean', 'cls', 'last', or 'attention'."
    }
}